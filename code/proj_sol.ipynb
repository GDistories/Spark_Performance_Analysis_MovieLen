{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 17:35:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "# 初始化 Spark 会话\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"MovieLens\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"4g\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "# 读取评分数据\n",
    "ratings_df = spark.read.csv('local-workspace/ml-25m/ratings.csv', header=True, inferSchema=True)\n",
    "movies_df = spark.read.csv('local-workspace/ml-25m/movies.csv', header=True, inferSchema=True)\n",
    "genome_scores_df = spark.read.csv('local-workspace/ml-25m/genome-scores.csv', header=True, inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:36:20.208100500Z",
     "start_time": "2023-12-04T17:35:52.879886600Z"
    }
   },
   "id": "4689b2092611103e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==================================================>    (185 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------+------------+\n",
      "|movieId|               title|average_rating|rating_count|\n",
      "+-------+--------------------+--------------+------------+\n",
      "| 204012|Kick That Habit (...|           5.0|           1|\n",
      "| 122193|   Kit Carson (1940)|           5.0|           1|\n",
      "| 202181| Warlock Moon (1973)|           5.0|           1|\n",
      "| 159471|Evening's Civil T...|           5.0|           1|\n",
      "| 131628|       Loaded (2014)|           5.0|           1|\n",
      "+-------+--------------------+--------------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "from pyspark.sql.functions import split, col\n",
    "# 计算评级数\n",
    "average_ratings = ratings_df.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
    "# 计算每部电影的评分人数\n",
    "rating_counts = ratings_df.groupBy(\"movieId\").agg(count(\"userId\").alias(\"rating_count\"))\n",
    "# 合并平均评分和评分人数\n",
    "movie_ratings = average_ratings.join(rating_counts, \"movieId\")\n",
    "\n",
    "# 将电影标题加入结果中\n",
    "movies_with_titles = movie_ratings.join(movies_df, \"movieId\").select(\"movieId\", \"title\", \"average_rating\", \"rating_count\")\n",
    "# 按平均评分降序排序\n",
    "sorted_movie_ratings = movies_with_titles.orderBy(\"average_rating\", ascending=False)\n",
    "# 显示结果\n",
    "sorted_movie_ratings.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:36:40.564317800Z",
     "start_time": "2023-12-04T17:36:20.209671500Z"
    }
   },
   "id": "fee772e9e3946518"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:====================================>                 (137 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+------------+\n",
      "|movieId|               title|   average_rating|rating_count|\n",
      "+-------+--------------------+-----------------+------------+\n",
      "| 171011|Planet Earth II (...|4.483096085409253|        1124|\n",
      "| 159817| Planet Earth (2006)|4.464796794504865|        1747|\n",
      "|    318|Shawshank Redempt...|4.413576004516335|       81482|\n",
      "| 170705|Band of Brothers ...|4.398598820058997|        1356|\n",
      "| 158958|    Pollyanna (2003)|4.384615384615385|          13|\n",
      "+-------+--------------------+-----------------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 筛选出评分人数大于 10 的电影\n",
    "movies_with_more_than_10_ratings = movies_with_titles.filter(movie_ratings.rating_count > 10)\n",
    "# 按平均评分降序排序\n",
    "sorted_movie_ratings = movies_with_more_than_10_ratings.orderBy(\"average_rating\", ascending=False)\n",
    "# 显示结果\n",
    "sorted_movie_ratings.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:36:57.762483600Z",
     "start_time": "2023-12-04T17:36:40.565896200Z"
    }
   },
   "id": "da9aa5209df99738"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===================================================>  (190 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------+\n",
      "|movieId|               title|    average_rating|rating_count|\n",
      "+-------+--------------------+------------------+------------+\n",
      "| 171011|Planet Earth II (...| 4.483096085409253|        1124|\n",
      "| 159817| Planet Earth (2006)| 4.464796794504865|        1747|\n",
      "|    318|Shawshank Redempt...| 4.413576004516335|       81482|\n",
      "| 170705|Band of Brothers ...| 4.398598820058997|        1356|\n",
      "| 171495|              Cosmos|4.3267148014440435|         277|\n",
      "+-------+--------------------+------------------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 筛选出评分人数大于 100 的电影\n",
    "movies_with_more_than_100_ratings = movies_with_titles.filter(movie_ratings.rating_count > 100)\n",
    "# 按平均评分降序排序\n",
    "sorted_movie_ratings = movies_with_more_than_100_ratings.orderBy(\"average_rating\", ascending=False)\n",
    "# 显示结果\n",
    "sorted_movie_ratings.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:37:14.527932100Z",
     "start_time": "2023-12-04T17:36:57.763708600Z"
    }
   },
   "id": "5031437f601798b0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:====================================================> (195 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+------------+\n",
      "|movieId|               title|   average_rating|rating_count|\n",
      "+-------+--------------------+-----------------+------------+\n",
      "| 171011|Planet Earth II (...|4.483096085409253|        1124|\n",
      "| 159817| Planet Earth (2006)|4.464796794504865|        1747|\n",
      "|    318|Shawshank Redempt...|4.413576004516335|       81482|\n",
      "| 170705|Band of Brothers ...|4.398598820058997|        1356|\n",
      "|    858|Godfather, The (1...|4.324336165187245|       52498|\n",
      "+-------+--------------------+-----------------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 筛选出评分人数大于 1000 的电影\n",
    "movies_with_more_than_1000_ratings = movies_with_titles.filter(movie_ratings.rating_count > 1000)\n",
    "# 按平均评分降序排序\n",
    "sorted_movie_ratings = movies_with_more_than_1000_ratings.orderBy(\"average_rating\", ascending=False)\n",
    "# 显示结果\n",
    "sorted_movie_ratings.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:37:32.130964900Z",
     "start_time": "2023-12-04T17:37:14.530033400Z"
    }
   },
   "id": "723ac7b4f0bd7750"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Splitting the Dataset:\n",
    "First, import the necessary modules and read your rating data into a Spark DataFrame. Then, split this data into training and testing sets."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T05:54:59.599548600Z",
     "start_time": "2023-12-02T05:54:59.579402Z"
    }
   },
   "id": "cb2d1f6a125fdb73"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.regression import LinearRegression"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:37:32.215643600Z",
     "start_time": "2023-12-04T17:37:32.133599Z"
    }
   },
   "id": "9b019011ede7ce52"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Split the data\n",
    "(train_data, test_data) = ratings_df.randomSplit([0.7, 0.3], seed=5021)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:37:32.215643600Z",
     "start_time": "2023-12-04T17:37:32.173037700Z"
    }
   },
   "id": "a9c4138157e3bfc2"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    296|   5.0|1147880044|\n",
      "|     1|    665|   5.0|1147878820|\n",
      "|     1|   1088|   4.0|1147868495|\n",
      "|     1|   1175|   3.5|1147868826|\n",
      "|     1|   1237|   5.0|1147868839|\n",
      "|     1|   1250|   4.0|1147868414|\n",
      "|     1|   1260|   3.5|1147877857|\n",
      "|     1|   2011|   2.5|1147868079|\n",
      "|     1|   2068|   2.5|1147869044|\n",
      "|     1|   2161|   3.5|1147868609|\n",
      "|     1|   2573|   4.0|1147878923|\n",
      "|     1|   2692|   5.0|1147869100|\n",
      "|     1|   3448|   4.0|1147868480|\n",
      "|     1|   3569|   5.0|1147879603|\n",
      "|     1|   3949|   5.0|1147868678|\n",
      "|     1|   4144|   5.0|1147868898|\n",
      "|     1|   4325|   5.0|1147878122|\n",
      "|     1|   4422|   3.0|1147869048|\n",
      "|     1|   4703|   4.0|1147869223|\n",
      "|     1|   5269|   0.5|1147879571|\n",
      "+------+-------+------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:37:36.609930100Z",
     "start_time": "2023-12-04T17:37:32.190048600Z"
    }
   },
   "id": "f262d8c32a3a667a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "|summary|           userId|           movieId|            rating|           timestamp|\n",
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "|  count|          7502892|           7502892|           7502892|             7502892|\n",
      "|   mean|81188.96771058413| 21366.92766642516|3.5339601716244884|1.2155145892096126E9|\n",
      "| stddev|46789.99952670272|39174.464718786876|1.0608207109360328| 2.268376646835225E8|\n",
      "|    min|                1|                 1|               0.5|           789652009|\n",
      "|    max|           162541|            209169|               5.0|          1574327549|\n",
      "+-------+-----------------+------------------+------------------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:37:47.456724300Z",
     "start_time": "2023-12-04T17:37:36.608855300Z"
    }
   },
   "id": "5a161ffa7cd8ca1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Matrix Factorization with ALS:\n",
    "ALS (Alternating Least Squares) is a popular matrix factorization algorithm in Spark's MLlib for collaborative filtering."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T05:56:30.049542500Z",
     "start_time": "2023-12-02T05:56:30.013347700Z"
    }
   },
   "id": "fea4ff3df3444541"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ALS_start_time = time.time()\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "als_model = als.fit(train_data)\n",
    "# Predictions\n",
    "predictions = als_model.transform(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T19:44:38.001305300Z",
     "start_time": "2023-12-04T19:42:30.998475900Z"
    }
   },
   "id": "c67bac7e8dcb86b"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 从 predictions 中选择需要的列\n",
    "selected_data = predictions.select(\"userId\", \"movieId\", \"rating\", \"prediction\")\n",
    "\n",
    "# 指定保存 CSV 文件的路径\n",
    "output_path = \"local-workspace/ml-25m/als_predictions.csv\"\n",
    "\n",
    "# 将 DataFrame 保存为 CSV 文件\n",
    "selected_data.coalesce(1).write.csv(path=output_path, mode=\"overwrite\", header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T19:57:09.230051400Z",
     "start_time": "2023-12-04T19:55:12.845299600Z"
    }
   },
   "id": "7f605d6c41561535"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.8228278220248162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:==================================================>   (188 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.3982120117710436\n",
      "Time used for ALS: 72.76677417755127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"r2\")\n",
    "test_eval = evaluator.evaluate(predictions)\n",
    "print(\"R2: {}\".format(test_eval))\n",
    "ALS_end_time = time.time()\n",
    "print(\"Time used for ALS: {}\".format(ALS_end_time - ALS_start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T17:38:20.217697600Z"
    }
   },
   "id": "33b57b4a31e11eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Extract Features and Build Another ML Model:\n",
    "For this part, you need to join your user and movie data with the rating data, then transform these features into a format suitable for machine learning models in Spark."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b78617ca9c0abb5d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 17:39:13 WARN Instrumentation: [c9a4855a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/12/04 17:39:14 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "23/12/04 17:39:32 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "23/12/04 17:39:32 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/12/04 17:39:32 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/12/04 17:39:32 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/12/04 17:39:32 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "23/12/04 17:39:33 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "23/12/04 17:39:49 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming movies_df and users_df are your DataFrames for movies and users\n",
    "# Join them with the rating data and perform feature transformations\n",
    "\n",
    "# An example of a feature transformation\n",
    "lr_start_time = time.time()\n",
    "string_indexer = StringIndexer(inputCol=\"userId\", outputCol=\"userIdIndex\")\n",
    "vector_assembler = VectorAssembler(inputCols=[\"userIdIndex\", \"movieId\"], outputCol=\"features\")\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"rating\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[string_indexer, vector_assembler, lr])\n",
    "lr_model = pipeline.fit(train_data)\n",
    "\n",
    "# predictions\n",
    "lr_predictions = lr_model.transform(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:39:59.757685100Z",
     "start_time": "2023-12-04T17:39:00.229752800Z"
    }
   },
   "id": "d0998b488f26938c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 17:39:59 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "23/12/04 17:40:13 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.051967075754656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 17:40:13 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "[Stage 107:===============================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.016622260944419875\n",
      "Time used for Linear Regression: 86.7550299167633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 17:40:26 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# evaluations\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "test_eval = evaluator.evaluate(lr_predictions)\n",
    "print(\"RMSE: {}\".format(test_eval))\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"r2\")\n",
    "test_eval = evaluator.evaluate(lr_predictions)\n",
    "print(\"R2: {}\".format(test_eval))\n",
    "lr_end_time = time.time()\n",
    "print(\"Time used for Linear Regression: {}\".format(lr_end_time - lr_start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:40:26.988422600Z",
     "start_time": "2023-12-04T17:39:59.760420200Z"
    }
   },
   "id": "1b6d6eb9af49b149"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deal with the feature of movies\n",
    "Count the average score of each movie\n",
    "Get the one-hot encoding of genres"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "278e89c9e444ce55"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 116:===============================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|movieId|movie_average_rating|movie_rating_count|\n",
      "+-------+--------------------+------------------+\n",
      "|    148|   2.908955223880597|               335|\n",
      "|    463|   2.813008130081301|               369|\n",
      "|    471|  3.6579813752234034|             10631|\n",
      "|    496|  3.2767624020887727|               383|\n",
      "|    833|  2.7182422451994093|              1354|\n",
      "+-------+--------------------+------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import avg, col\n",
    "data_pre_process_start_time = time.time()\n",
    "# 求所有电影的平均评分\n",
    "average_rating_df = movies_with_titles.select(\"movieId\", \"average_rating\", \"rating_count\") # 前面已经算过\n",
    "average_rating_df = average_rating_df.withColumnRenamed(\"average_rating\", \"movie_average_rating\")\n",
    "average_rating_df = average_rating_df.withColumnRenamed(\"rating_count\", \"movie_rating_count\")\n",
    "\n",
    "# 用均分填补没有打分的电影\n",
    "overall_average_rating = average_rating_df.select(avg(\"movie_average_rating\")).first()[0]\n",
    "average_rating_df = average_rating_df.na.fill({\"movie_average_rating\": overall_average_rating})\n",
    "\n",
    "# 显示前5条记录\n",
    "average_rating_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:01.549767Z",
     "start_time": "2023-12-04T17:40:26.993407900Z"
    }
   },
   "id": "4086b7521321b9ff"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# 拆分 genres 字段\n",
    "movies_df_vectorized = movies_df.withColumn(\"split_genres\", split(col(\"genres\"), \"\\|\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:01.565832500Z",
     "start_time": "2023-12-04T17:41:01.549767Z"
    }
   },
   "id": "5393ace19a96c032"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "# 获取所有可能的类别\n",
    "all_genres = movies_df_vectorized.select(explode(col(\"split_genres\")).alias(\"genre\")).distinct().collect()\n",
    "all_genres = [row['genre'] for row in all_genres]\n",
    "\n",
    "# 为每个类别定义一个 UDF\n",
    "def genre_indicator(genre):\n",
    "    def indicator(genres_list):\n",
    "        return 1 if genre in genres_list else 0\n",
    "    return udf(indicator, IntegerType())\n",
    "\n",
    "# 为每个类别添加一个新列\n",
    "for genre in all_genres:\n",
    "    genre_udf = genre_indicator(genre)\n",
    "    movies_df_vectorized = movies_df_vectorized.withColumn(genre, genre_udf(col(\"split_genres\")))\n",
    "movies_df_vectorized = movies_df_vectorized.drop(\"title\", \"genres\", \"split_genres\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:02.463363400Z",
     "start_time": "2023-12-04T17:41:01.564672300Z"
    }
   },
   "id": "f38510edb24129d2"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+--------+---------+-----+---+-----------+-------+-------+-------+---------+---------+------------------+----+------+-------+------+--------+------+------+\n",
      "|movieId|Crime|Romance|Thriller|Adventure|Drama|War|Documentary|Fantasy|Mystery|Musical|Animation|Film-Noir|(no genres listed)|IMAX|Horror|Western|Comedy|Children|Action|Sci-Fi|\n",
      "+-------+-----+-------+--------+---------+-----+---+-----------+-------+-------+-------+---------+---------+------------------+----+------+-------+------+--------+------+------+\n",
      "|      1|    0|      0|       0|        1|    0|  0|          0|      1|      0|      0|        1|        0|                 0|   0|     0|      0|     1|       1|     0|     0|\n",
      "|      2|    0|      0|       0|        1|    0|  0|          0|      1|      0|      0|        0|        0|                 0|   0|     0|      0|     0|       1|     0|     0|\n",
      "|      3|    0|      1|       0|        0|    0|  0|          0|      0|      0|      0|        0|        0|                 0|   0|     0|      0|     1|       0|     0|     0|\n",
      "|      4|    0|      1|       0|        0|    1|  0|          0|      0|      0|      0|        0|        0|                 0|   0|     0|      0|     1|       0|     0|     0|\n",
      "|      5|    0|      0|       0|        0|    0|  0|          0|      0|      0|      0|        0|        0|                 0|   0|     0|      0|     1|       0|     0|     0|\n",
      "+-------+-----+-------+--------+---------+-----+---+-----------+-------+-------+-------+---------+---------+------------------+----+------+-------+------+--------+------+------+\n"
     ]
    }
   ],
   "source": [
    "movies_df_vectorized.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:02.953209700Z",
     "start_time": "2023-12-04T17:41:02.464439300Z"
    }
   },
   "id": "5147f617359ab130"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Another vectorized method of movie genre\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d68d34207ac58692"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+--------+------+-------+------+------+------+------+-------+\n",
      "|movieId|   genre1|   genre2|  genre3|genre4| genre5|genre6|genre7|genre8|genre9|genre10|\n",
      "+-------+---------+---------+--------+------+-------+------+------+------+------+-------+\n",
      "|      1|Adventure|Animation|Children|Comedy|Fantasy|  null|  null|  null|  null|   null|\n",
      "|      2|Adventure| Children| Fantasy|  null|   null|  null|  null|  null|  null|   null|\n",
      "|      3|   Comedy|  Romance|    null|  null|   null|  null|  null|  null|  null|   null|\n",
      "|      4|   Comedy|    Drama| Romance|  null|   null|  null|  null|  null|  null|   null|\n",
      "|      5|   Comedy|     null|    null|  null|   null|  null|  null|  null|  null|   null|\n",
      "+-------+---------+---------+--------+------+-------+------+------+------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "max_array_length = 10\n",
    "# 假设 movie_df 是您的原始 DataFrame，并且 genres 列包含由 '|' 分隔的字符串\n",
    "# 将 genres 列拆分为数组\n",
    "split_genres = split(movies_df['genres'], '\\|')\n",
    "\n",
    "new_columns = [movies_df['movieId']]  # 包括 movieId 列\n",
    "# 使用列表推导式创建新列\n",
    "new_columns += [split_genres.getItem(i).alias(f'genre{i+1}') for i in range(max_array_length)]\n",
    "# 创建新的 DataFrame，仅包含这些 genre 列\n",
    "genre_df = movies_df.select(*new_columns)\n",
    "# 显示结果\n",
    "genre_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:03.141451300Z",
     "start_time": "2023-12-04T17:41:02.957147400Z"
    }
   },
   "id": "4067153454446dff"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+\n",
      "|movieId|genre1Index|genre2Index|genre3Index|genre4Index|genre5Index|genre6Index|genre7Index|genre8Index|genre9Index|genre10Index|\n",
      "+-------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+\n",
      "|      1|        7.0|       12.0|        9.0|        5.0|        3.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|      2|        7.0|       10.0|        5.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|      3|        1.0|        1.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|      4|        1.0|        0.0|        1.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|      5|        1.0|       18.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|      6|        2.0|        5.0|        0.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|      7|        1.0|        1.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|      8|        7.0|       10.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|      9|        2.0|       18.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     10|        2.0|        6.0|        0.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     11|        1.0|        0.0|        1.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     12|        1.0|        4.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     13|        7.0|       12.0|        9.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     14|        0.0|       18.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     15|        2.0|        6.0|        1.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     16|        6.0|        0.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     17|        0.0|        1.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     18|        1.0|       18.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     19|        1.0|       18.0|       17.0|       16.0|       13.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "|     20|        2.0|        3.0|       10.0|        6.0|        0.0|       11.0|        8.0|        3.0|        1.0|         1.0|\n",
      "+-------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+\n"
     ]
    }
   ],
   "source": [
    "# 进行indexer操作\n",
    "indexers = [StringIndexer(inputCol=f'genre{i+1}', outputCol=f'genre{i+1}Index', handleInvalid=\"keep\") for i in range(max_array_length)]\n",
    "# 创建 Pipeline\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "# 应用 Pipeline\n",
    "indexed_df = pipeline.fit(genre_df).transform(genre_df)\n",
    "selected_columns = [col for col in indexed_df.columns if not col.startswith('genre') or col.endswith('Index')]\n",
    "genre_indexed_df = indexed_df.select(selected_columns)\n",
    "# 显示结果\n",
    "genre_indexed_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:05.530730500Z",
     "start_time": "2023-12-04T17:41:03.095448200Z"
    }
   },
   "id": "f81029406e40e218"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deal with the feature of users\n",
    "calculate the average rating of each user\n",
    "calculate the average rating of each user in each kind of movie\n",
    "calculate the max point and the min point of each user"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b15d38957b8ab018"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 143:===============================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|userId|user_average_rating|\n",
      "+------+-------------------+\n",
      "| 33375|   3.81651376146789|\n",
      "| 33412|  4.336956521739131|\n",
      "| 33569| 3.7745098039215685|\n",
      "| 33602|  2.212121212121212|\n",
      "| 33717|   3.38031914893617|\n",
      "+------+-------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 假设 rating_df 已经是一个 PySpark DataFrame\n",
    "# 计算每个用户的电影平均评分\n",
    "user_average_ratings = ratings_df.groupBy(\"userId\").agg(avg(\"rating\").alias(\"user_average_rating\"))\n",
    "# 显示结果\n",
    "user_average_ratings.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:14.119482200Z",
     "start_time": "2023-12-04T17:41:05.533360400Z"
    }
   },
   "id": "661ba3541a433a7"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "big_df = ratings_df.join(user_average_ratings, on=\"userId\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:14.141088800Z",
     "start_time": "2023-12-04T17:41:14.118341500Z"
    }
   },
   "id": "a44de6bbfb6ba03d"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "big_df = big_df.join(movies_df_vectorized, on=\"movieId\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:14.141592100Z",
     "start_time": "2023-12-04T17:41:14.128883Z"
    }
   },
   "id": "cc5c811d1e6d6172"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, avg\n",
    "\n",
    "# 假设 big_df 已经是一个 PySpark DataFrame\n",
    "# 列出所有的电影类型\n",
    "genres = [\"Crime\", \"Romance\", \"Thriller\", \"Adventure\", \"Drama\", \"War\", \n",
    "          \"Documentary\", \"Fantasy\", \"Mystery\", \"Musical\", \"Animation\", \n",
    "          \"Film-Noir\", \"(no genres listed)\", \"IMAX\", \"Horror\", \"Western\", \n",
    "          \"Comedy\", \"Children\", \"Action\", \"Sci-Fi\"]\n",
    "\n",
    "# 计算每个用户对每个类型的平均评分\n",
    "exprs = [avg(when(col(genre) == 1, col(\"rating\"))).alias(genre) for genre in genres]\n",
    "\n",
    "user_genre_ratings = big_df.groupBy(\"userId\").agg(*exprs)\n",
    "user_genre_ratings = user_genre_ratings.join(user_average_ratings, \"userId\")\n",
    "\n",
    "\n",
    "# 填充缺失值\n",
    "for genre in genres:\n",
    "    user_genre_ratings = user_genre_ratings.withColumn(genre, when(col(genre).isNull(), col(\"user_average_rating\")).otherwise(col(genre)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:14.517119500Z",
     "start_time": "2023-12-04T17:41:14.138387500Z"
    }
   },
   "id": "e5f0731712fa231b"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 改column名方便确认这部分特征是来自用户的\n",
    "for genre in genres:\n",
    "    user_genre_ratings = user_genre_ratings.withColumnRenamed(genre, \"user_\" + genre)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:14.577715600Z",
     "start_time": "2023-12-04T17:41:14.518166400Z"
    }
   },
   "id": "40d8be910b21146c"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 17:41:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 148:===============================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-----------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|userId|        user_Crime|     user_Romance|     user_Thriller|   user_Adventure|        user_Drama|          user_War|  user_Documentary|     user_Fantasy|      user_Mystery|      user_Musical|    user_Animation|    user_Film-Noir|user_(no genres listed)|         user_IMAX|       user_Horror|      user_Western|       user_Comedy|     user_Children|       user_Action|       user_Sci-Fi|user_average_rating|\n",
      "+------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-----------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|   148| 4.205882352941177|4.023809523809524| 4.162162162162162|4.089285714285714|            4.1375|            4.3125|              4.25|4.066666666666666| 4.222222222222222|3.8333333333333335|3.9285714285714284| 4.222222222222222|      4.128787878787879|3.9285714285714284|              4.25| 4.214285714285714|              3.95| 3.857142857142857|          4.109375|           4.03125|  4.128787878787879|\n",
      "|   463| 4.333333333333333|             3.75|               4.2|              4.5|               5.0|3.9047619047619047|3.9047619047619047|              3.0|               4.0|               2.0|3.9047619047619047|3.9047619047619047|     3.9047619047619047|3.9047619047619047|               4.0|3.9047619047619047|               3.4|               2.0|               4.4|               4.2| 3.9047619047619047|\n",
      "|   471|              3.75|4.142857142857143|               4.0|4.166666666666667| 4.071428571428571| 4.166666666666667| 4.051282051282051|4.136363636363637|               5.0|               4.0|               4.0| 4.051282051282051|      4.051282051282051| 4.035714285714286|               4.0| 4.051282051282051|3.9615384615384617|               4.0|           3.90625|3.9444444444444446|  4.051282051282051|\n",
      "|   496|               3.5|4.357142857142857|               4.5|              4.5|4.2105263157894735|3.8870967741935485|3.8870967741935485|            3.125|3.8870967741935485| 4.555555555555555|               5.0|3.8870967741935485|     3.8870967741935485|               3.0|               0.5|3.8870967741935485|3.7142857142857144|              4.25|             2.875| 4.333333333333333| 3.8870967741935485|\n",
      "|   833|3.7941176470588234|3.293103448275862|3.2794117647058822|3.507936507936508|3.4754098360655736|3.5833333333333335|               4.0|              3.0|3.4615384615384617|             3.125|3.9107142857142856|               3.5|     3.4778481012658227|3.1785714285714284|3.6538461538461537|               3.5| 3.638888888888889|3.9038461538461537|3.4583333333333335| 3.357142857142857| 3.4778481012658227|\n",
      "+------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-----------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_genre_ratings.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:39.721099400Z",
     "start_time": "2023-12-04T17:41:14.579473Z"
    }
   },
   "id": "88247131b4bc7916"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, avg, max, min\n",
    "# 计算每个用户的最高分和最低分\n",
    "user_max_min_ratings = big_df.groupBy(\"userId\").agg(\n",
    "    max(\"rating\").alias(\"user_max\"),\n",
    "    min(\"rating\").alias(\"user_min\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:39.744040500Z",
     "start_time": "2023-12-04T17:41:39.723746400Z"
    }
   },
   "id": "d615aba6d9bea4ad"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 152:===============================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+\n",
      "|userId|user_max|user_min|\n",
      "+------+--------+--------+\n",
      "|   148|     5.0|     3.0|\n",
      "|   463|     5.0|     1.0|\n",
      "|   471|     5.0|     3.0|\n",
      "|   496|     5.0|     0.5|\n",
      "|   833|     5.0|     0.5|\n",
      "+------+--------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_max_min_ratings.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:56.921055300Z",
     "start_time": "2023-12-04T17:41:39.746205200Z"
    }
   },
   "id": "6ec6f6e287646dfa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Join feature matrix and genome_scores by movieId "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e32cdc0106ad5bee"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "big_df = big_df.join(average_rating_df, on=\"movieId\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:56.939418500Z",
     "start_time": "2023-12-04T17:41:56.922602900Z"
    }
   },
   "id": "3999eb495b2cab6e"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "big_df = big_df.drop('user_average_rating')\n",
    "big_df = big_df.join(user_genre_ratings, on=\"userId\")\n",
    "big_df = big_df.join(user_max_min_ratings, on=\"userId\")\n",
    "big_df = big_df.join(movies_df, on=\"movieId\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:57.089129700Z",
     "start_time": "2023-12-04T17:41:56.937807Z"
    }
   },
   "id": "731bafc9cbf022f0"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+-----+-------+--------+---------+-----+---+-----------+-------+-------+-------+---------+---------+------------------+----+------+-------+------+--------+------+------+--------------------+------------------+-----------------+-----------------+-----------------+-----------------+----------+--------+----------------+-----------------+-----------------+------------------+------------------+-----------------+-----------------------+------------------+-----------+-----------------+-----------+-----------------+-----------+-----------+-------------------+--------+--------+--------------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|Crime|Romance|Thriller|Adventure|Drama|War|Documentary|Fantasy|Mystery|Musical|Animation|Film-Noir|(no genres listed)|IMAX|Horror|Western|Comedy|Children|Action|Sci-Fi|movie_average_rating|movie_rating_count|       user_Crime|     user_Romance|    user_Thriller|   user_Adventure|user_Drama|user_War|user_Documentary|     user_Fantasy|     user_Mystery|      user_Musical|    user_Animation|   user_Film-Noir|user_(no genres listed)|         user_IMAX|user_Horror|     user_Western|user_Comedy|    user_Children|user_Action|user_Sci-Fi|user_average_rating|user_max|user_min|               title|              genres|\n",
      "+-------+------+------+----------+-----+-------+--------+---------+-----+---+-----------+-------+-------+-------+---------+---------+------------------+----+------+-------+------+--------+------+------+--------------------+------------------+-----------------+-----------------+-----------------+-----------------+----------+--------+----------------+-----------------+-----------------+------------------+------------------+-----------------+-----------------------+------------------+-----------+-----------------+-----------+-----------------+-----------+-----------+-------------------+--------+--------+--------------------+--------------------+\n",
      "|    296|   148|   5.0|1454942620|    1|      0|       1|        0|    1|  0|          0|      0|      0|      0|        0|        0|                 0|   0|     0|      0|     1|       0|     0|     0|   4.188912039361382|             79672|4.205882352941177|4.023809523809524|4.162162162162162|4.089285714285714|    4.1375|  4.3125|            4.25|4.066666666666666|4.222222222222222|3.8333333333333335|3.9285714285714284|4.222222222222222|      4.128787878787879|3.9285714285714284|       4.25|4.214285714285714|       3.95|3.857142857142857|   4.109375|    4.03125|  4.128787878787879|     5.0|     3.0| Pulp Fiction (1994)|Comedy|Crime|Dram...|\n",
      "|  81834|   148|   3.5|1454942987|    0|      0|       0|        1|    0|  0|          0|      1|      0|      0|        0|        0|                 0|   1|     0|      0|     0|       0|     1|     0|  3.8654379847719347|             14053|4.205882352941177|4.023809523809524|4.162162162162162|4.089285714285714|    4.1375|  4.3125|            4.25|4.066666666666666|4.222222222222222|3.8333333333333335|3.9285714285714284|4.222222222222222|      4.128787878787879|3.9285714285714284|       4.25|4.214285714285714|       3.95|3.857142857142857|   4.109375|    4.03125|  4.128787878787879|     5.0|     3.0|Harry Potter and ...|Action|Adventure|...|\n",
      "| 116897|   148|   4.0|1454942698|    0|      0|       1|        0|    1|  0|          0|      0|      0|      0|        0|        0|                 0|   0|     0|      0|     1|       0|     0|     0|   4.105126877265666|              1931|4.205882352941177|4.023809523809524|4.162162162162162|4.089285714285714|    4.1375|  4.3125|            4.25|4.066666666666666|4.222222222222222|3.8333333333333335|3.9285714285714284|4.222222222222222|      4.128787878787879|3.9285714285714284|       4.25|4.214285714285714|       3.95|3.857142857142857|   4.109375|    4.03125|  4.128787878787879|     5.0|     3.0|   Wild Tales (2014)|Comedy|Drama|Thri...|\n",
      "|    924|   148|   5.0|1454942880|    0|      0|       0|        1|    1|  0|          0|      0|      0|      0|        0|        0|                 0|   0|     0|      0|     0|       0|     0|     1|   3.981349757113116|             28820|4.205882352941177|4.023809523809524|4.162162162162162|4.089285714285714|    4.1375|  4.3125|            4.25|4.066666666666666|4.222222222222222|3.8333333333333335|3.9285714285714284|4.222222222222222|      4.128787878787879|3.9285714285714284|       4.25|4.214285714285714|       3.95|3.857142857142857|   4.109375|    4.03125|  4.128787878787879|     5.0|     3.0|2001: A Space Ody...|Adventure|Drama|S...|\n",
      "|     47|   148|   4.0|1454943207|    0|      0|       1|        0|    0|  0|          0|      0|      1|      0|        0|        0|                 0|   0|     0|      0|     0|       0|     0|     0|  4.0791663372598626|             50596|4.205882352941177|4.023809523809524|4.162162162162162|4.089285714285714|    4.1375|  4.3125|            4.25|4.066666666666666|4.222222222222222|3.8333333333333335|3.9285714285714284|4.222222222222222|      4.128787878787879|3.9285714285714284|       4.25|4.214285714285714|       3.95|3.857142857142857|   4.109375|    4.03125|  4.128787878787879|     5.0|     3.0|Seven (a.k.a. Se7...|    Mystery|Thriller|\n",
      "+-------+------+------+----------+-----+-------+--------+---------+-----+---+-----------+-------+-------+-------+---------+---------+------------------+----+------+-------+------+--------+------+------+--------------------+------------------+-----------------+-----------------+-----------------+-----------------+----------+--------+----------------+-----------------+-----------------+------------------+------------------+-----------------+-----------------------+------------------+-----------+-----------------+-----------+-----------------+-----------+-----------+-------------------+--------+--------+--------------------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "big_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:43:06.893209800Z",
     "start_time": "2023-12-04T17:41:57.033683400Z"
    }
   },
   "id": "902fb1c20435c7ce"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used for data pre-process: 159.901447057724\n"
     ]
    }
   ],
   "source": [
    "data_pre_process_end_time = time.time()\n",
    "print(\"Time used for data pre-process: {}\".format(data_pre_process_end_time - data_pre_process_start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:43:06.898700800Z",
     "start_time": "2023-12-04T17:43:06.894546100Z"
    }
   },
   "id": "74c3739957806be9"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# # Data Alignment Using another way of vectorizing\n",
    "# big_df = big_df.join(genre_df, on=\"movieId\")\n",
    "# big_df = big_df.join(user_average_ratings, on=\"userId\")\n",
    "# big_df = big_df.join(average_rating_df, on=\"movieId\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:43:06.922173700Z",
     "start_time": "2023-12-04T17:43:06.898700800Z"
    }
   },
   "id": "a1977b2a71c42978"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data Alignment using only average rating of users and movies\n",
    "big_df = ratings_df.join(user_average_ratings, on=\"userId\")\n",
    "big_df = big_df.join(average_rating_df, on=\"movieId\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2649c29cd95965ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using ML model to predict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be7c02741ce11e15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use Linear Regression to predict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1f2d3548aac2bc6"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "(train_data, test_data) = big_df.randomSplit([0.7, 0.3], seed=5021)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T18:54:11.885213700Z",
     "start_time": "2023-12-04T18:54:11.862599900Z"
    }
   },
   "id": "38bc86cd8eee3398"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "['Crime',\n 'Romance',\n 'Thriller',\n 'Adventure',\n 'Drama',\n 'War',\n 'Documentary',\n 'Fantasy',\n 'Mystery',\n 'Musical',\n 'Animation',\n 'Film-Noir',\n '(no genres listed)',\n 'IMAX',\n 'Horror',\n 'Western',\n 'Comedy',\n 'Children',\n 'Action',\n 'Sci-Fi',\n 'movie_average_rating',\n 'movie_rating_count',\n 'user_Crime',\n 'user_Romance',\n 'user_Thriller',\n 'user_Adventure',\n 'user_Drama',\n 'user_War',\n 'user_Documentary',\n 'user_Fantasy',\n 'user_Mystery',\n 'user_Musical',\n 'user_Animation',\n 'user_Film-Noir',\n 'user_(no genres listed)',\n 'user_IMAX',\n 'user_Horror',\n 'user_Western',\n 'user_Comedy',\n 'user_Children',\n 'user_Action',\n 'user_Sci-Fi',\n 'user_average_rating',\n 'user_max',\n 'user_min']"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_final_start_time = time.time()\n",
    "# Assuming movies_df and users_df are your DataFrames for movies and users\n",
    "# Join them with the rating data and perform feature transformations\n",
    "input_cols = big_df.columns\n",
    "to_remove = [\"rating\", \"userId\", \"movieId\", \"timestamp\", \"title\", \"genres\"]\n",
    "# to_remove = [\"rating\", \"timestamp\", \"title\", \"genres\"]\n",
    "for col in to_remove:\n",
    "    if col in input_cols:\n",
    "        input_cols.remove(col)\n",
    "input_cols"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:43:06.986515800Z",
     "start_time": "2023-12-04T17:43:06.922703400Z"
    }
   },
   "id": "432d3462ef9c9a2e"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"rating\", regParam=0.1)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[vector_assembler, lr])\n",
    "lr_model = pipeline.fit(train_data)\n",
    "\n",
    "# predictions\n",
    "lr_predictions = lr_model.transform(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:48:38.265759500Z",
     "start_time": "2023-12-04T17:43:06.933247700Z"
    }
   },
   "id": "fa8f5cd5177b524b"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 从 predictions 中选择需要的列\n",
    "selected_data = lr_predictions.select(\"userId\", \"movieId\", \"rating\", \"prediction\")\n",
    "\n",
    "# 指定保存 CSV 文件的路径\n",
    "output_path = \"local-workspace/ml-25m/lr_predictions.csv\"\n",
    "\n",
    "# 将 DataFrame 保存为 CSV 文件\n",
    "selected_data.coalesce(1).write.csv(path=output_path, mode=\"overwrite\", header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T19:59:52.457137200Z",
     "start_time": "2023-12-04T19:57:09.232373400Z"
    }
   },
   "id": "603e6333f081aaab"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8651459535999295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 232:====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.3350825148025829\n",
      "Time used for Linear Regression: 537.2375557422638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# evaluations\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "test_eval = evaluator.evaluate(lr_predictions)\n",
    "print(\"RMSE: {}\".format(test_eval))\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"r2\")\n",
    "test_eval = evaluator.evaluate(lr_predictions)\n",
    "print(\"R2: {}\".format(test_eval))\n",
    "lr_final_end_time = time.time()\n",
    "print(\"Time used for Linear Regression: {}\".format(lr_final_end_time - lr_final_start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:52:04.161628900Z",
     "start_time": "2023-12-04T17:48:38.268450500Z"
    }
   },
   "id": "74487d34e128d4c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use random forest to predict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c216337b0fe5e0b1"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "\n",
    "rf_start_time = time.time()\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"rating\")\n",
    "\n",
    "# 创建管道\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# 训练模型\n",
    "model = pipeline.fit(train_data)\n",
    "# 预测\n",
    "predictions = model.transform(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:57:43.818925400Z",
     "start_time": "2023-12-04T17:52:04.164869500Z"
    }
   },
   "id": "265395bca3110144"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime: 6.886792801968188e-05\n",
      "Romance: 0.0\n",
      "Thriller: 3.523737173904606e-05\n",
      "Adventure: 0.00022844381095603352\n",
      "Drama: 0.004749294894781922\n",
      "War: 0.0\n",
      "Documentary: 0.0\n",
      "Fantasy: 0.0\n",
      "Mystery: 0.0\n",
      "Musical: 0.0\n",
      "Animation: 0.0\n",
      "Film-Noir: 0.0\n",
      "(no genres listed): 0.0\n",
      "IMAX: 0.0\n",
      "Horror: 0.0\n",
      "Western: 0.0\n",
      "Comedy: 0.00037033431206308414\n",
      "Children: 0.0\n",
      "Action: 0.0009589248997926818\n",
      "Sci-Fi: 6.0410468050548805e-05\n",
      "movie_average_rating: 0.36517223034791796\n",
      "movie_rating_count: 0.01081479597023588\n",
      "user_Crime: 0.023427844242746066\n",
      "user_Romance: 0.003515250931201064\n",
      "user_Thriller: 0.07693333620040771\n",
      "user_Adventure: 0.032349879822003685\n",
      "user_Drama: 0.0982825875963453\n",
      "user_War: 0.001984513888941274\n",
      "user_Documentary: 0.0015511478486050889\n",
      "user_Fantasy: 0.0023520137220515275\n",
      "user_Mystery: 0.001596039048151747\n",
      "user_Musical: 0.00023382565989862\n",
      "user_Animation: 0.0005719214261535914\n",
      "user_Film-Noir: 0.000504691361966573\n",
      "user_(no genres listed): 0.036662998952653875\n",
      "user_IMAX: 5.9601541639709355e-05\n",
      "user_Horror: 6.326573216569069e-05\n",
      "user_Western: 0.0\n",
      "user_Comedy: 0.0687309455067471\n",
      "user_Children: 0.0\n",
      "user_Action: 0.05667810016442689\n",
      "user_Sci-Fi: 0.0022339396931387585\n",
      "user_average_rating: 0.209809556657199\n",
      "user_max: 0.0\n",
      "user_min: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 获取训练好的随机森林模型\n",
    "rf_model = model.stages[-1]\n",
    "\n",
    "# 获取特征重要性\n",
    "importances = rf_model.featureImportances\n",
    "\n",
    "# 特征名称与其重要性\n",
    "for feature, importance in zip(input_cols, importances):\n",
    "    print(f\"{feature}: {importance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T17:57:43.908004400Z",
     "start_time": "2023-12-04T17:57:43.830613Z"
    }
   },
   "id": "d24d690cdf7104c6"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8762516875542676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 339:====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.31790207768881995\n",
      "Time used for Random Forest: 560.7150785923004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# evaluations\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "test_eval = evaluator.evaluate(predictions)\n",
    "print(\"RMSE: {}\".format(test_eval))\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"r2\")\n",
    "test_eval = evaluator.evaluate(predictions)\n",
    "print(\"R2: {}\".format(test_eval))\n",
    "rf_end_time = time.time()\n",
    "print(\"Time used for Random Forest: {}\".format(rf_end_time - rf_start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T18:01:24.883793700Z",
     "start_time": "2023-12-04T17:57:43.915566100Z"
    }
   },
   "id": "fbdb62aabe32668"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use ALS to predict(combining with feature matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16f238675f96d887"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 18:48:39 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: Column features must be of type numeric but was actually of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:78)\n",
      "\tat org.apache.spark.ml.recommendation.ALSParams.validateAndTransformSchema(ALS.scala:256)\n",
      "\tat org.apache.spark.ml.recommendation.ALSParams.validateAndTransformSchema$(ALS.scala:253)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.validateAndTransformSchema(ALS.scala:593)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.transformSchema(ALS.scala:725)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:692)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:691)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column features must be of type numeric but was actually of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m train_data \u001B[38;5;241m=\u001B[39m assembler\u001B[38;5;241m.\u001B[39mtransform(train_data)\n\u001B[1;32m      9\u001B[0m test_data \u001B[38;5;241m=\u001B[39m assembler\u001B[38;5;241m.\u001B[39mtransform(test_data)\n\u001B[0;32m---> 10\u001B[0m als_model \u001B[38;5;241m=\u001B[39m \u001B[43mals\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# 使用训练好的模型进行预测\u001B[39;00m\n\u001B[1;32m     12\u001B[0m predictions \u001B[38;5;241m=\u001B[39m als_model\u001B[38;5;241m.\u001B[39mtransform(test_data)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/ml/base.py:129\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    127\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 129\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    132\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params))\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/ml/wrapper.py:321\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset):\n\u001B[0;32m--> 321\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    322\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/ml/wrapper.py:318\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;124;03mFits a Java model to the input dataset.\u001B[39;00m\n\u001B[1;32m    311\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;124;03m:return: fitted Java model\u001B[39;00m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 318\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1304\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1298\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1299\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1300\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1301\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1303\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1304\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1305\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1308\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:137\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    133\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m     \u001B[43mraise_from\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconverted\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[0;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[0;34m(e)\u001B[0m\n",
      "\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column features must be of type numeric but was actually of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>>."
     ]
    }
   ],
   "source": [
    "# # 初始化计时\n",
    "# ALS_final_start_time = time.time()\n",
    "# # 特征列\n",
    "# feature_cols = ['movieId'] + genres  # 确保这里列出了所有相关的独热编码列\n",
    "# als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"features\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "# # 创建VectorAssembler\n",
    "# assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "# train_data = assembler.transform(train_data)\n",
    "# test_data = assembler.transform(test_data)\n",
    "# als_model = als.fit(train_data)\n",
    "# # 使用训练好的模型进行预测\n",
    "# predictions = als_model.transform(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T18:48:39.834850900Z",
     "start_time": "2023-12-04T18:48:39.728419300Z"
    }
   },
   "id": "dd1a8495b2482df1"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "21"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T18:49:58.939942300Z",
     "start_time": "2023-12-04T18:49:58.932388500Z"
    }
   },
   "id": "8a476d3f076254f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluations\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "test_eval = evaluator.evaluate(predictions)\n",
    "print(\"RMSE: {}\".format(test_eval))\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"r2\")\n",
    "test_eval = evaluator.evaluate(predictions)\n",
    "print(\"R2: {}\".format(test_eval))\n",
    "rf_end_time = time.time()\n",
    "print(\"Time used for Random Forest: {}\".format(rf_end_time - rf_start_time))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65a51477e859c92b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use GBT to predict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a475c81bdb0a628a"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "GBT_start_time = time.time()\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"rating\", maxIter=10)\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "model = pipeline.fit(train_data)\n",
    "predictions = model.transform(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T19:01:52.588384500Z",
     "start_time": "2023-12-04T18:56:07.645619900Z"
    }
   },
   "id": "7564328ce349ad1d"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0263213320434903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 899:====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.0642589035793959\n",
      "Time used for GBT: 557.7050244808197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# evaluations\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "test_eval = evaluator.evaluate(predictions)\n",
    "print(\"RMSE: {}\".format(test_eval))\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"r2\")\n",
    "test_eval = evaluator.evaluate(predictions)\n",
    "print(\"R2: {}\".format(test_eval))\n",
    "GBT_end_time = time.time()\n",
    "print(\"Time used for GBT: {}\".format(GBT_end_time - GBT_start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T19:05:25.353998900Z",
     "start_time": "2023-12-04T19:01:52.578443300Z"
    }
   },
   "id": "c0f4a75b51f71c9"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used for ALS: 72.76677417755127\n",
      "Time used for Linear Regression: 86.7550299167633\n",
      "Time used for data pre-process: 159.901447057724\n",
      "Time used for Linear Regression: 537.2375557422638\n",
      "Time used for Random Forest: 560.7150785923004\n",
      "Time used for GBT: 557.7050244808197\n"
     ]
    }
   ],
   "source": [
    "print(\"Time used for ALS: {}\".format(ALS_end_time - ALS_start_time))\n",
    "print(\"Time used for Linear Regression: {}\".format(lr_end_time - lr_start_time))\n",
    "print(\"Time used for data pre-process: {}\".format(data_pre_process_end_time - data_pre_process_start_time))\n",
    "print(\"Time used for Linear Regression: {}\".format(lr_final_end_time - lr_final_start_time))\n",
    "print(\"Time used for Random Forest: {}\".format(rf_end_time - rf_start_time))\n",
    "print(\"Time used for GBT: {}\".format(GBT_end_time - GBT_start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T19:05:25.369312800Z",
     "start_time": "2023-12-04T19:05:25.355587700Z"
    }
   },
   "id": "cfdbea41605c0dbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use Anotherway to make feature matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da6d431e2c073453"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# 提取所需列\n",
    "selected_columns = big_df.select(\"user_average_rating\", \"movie_average_rating\", \"movie_rating_count\")\n",
    "\n",
    "# 计算每部电影所属的 genre 数量\n",
    "genre_count = sum([col(genre) for genre in genres]).alias(\"genre_count\")\n",
    "big_df = big_df.withColumn(\"genre_count\", genre_count)\n",
    "\n",
    "# 为新 DataFrame 创建新特征\n",
    "for genre in genres:\n",
    "    user_avg_rating_col = f\"user_{genre}\"  # 用户对该 genre 的平均评分列名\n",
    "    genre_feature_col = f\"{genre}_feature\"  # 新的特征列名\n",
    "\n",
    "    # 计算特征值并添加到新 DataFrame\n",
    "    selected_columns = selected_columns.withColumn(\n",
    "        genre_feature_col,\n",
    "        when(big_df[genre] == 1, big_df[user_avg_rating_col] / big_df[\"genre_count\"]).otherwise(0)\n",
    "    )\n",
    "\n",
    "# 显示新 DataFrame 的结果\n",
    "selected_columns.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab98243c09b93173"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T19:05:25.704828500Z"
    }
   },
   "id": "c7544a6f0eb4831"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # evaluations\n",
    "# evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "# test_eval = evaluator.evaluate(predictions)\n",
    "# print(\"RMSE: {}\".format(test_eval))\n",
    "# evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"r2\")\n",
    "# test_eval = evaluator.evaluate(predictions)\n",
    "# print(\"R2: {}\".format(test_eval))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T19:05:25.708879300Z",
     "start_time": "2023-12-04T19:05:25.706774900Z"
    }
   },
   "id": "5d3916d3c374c8ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
